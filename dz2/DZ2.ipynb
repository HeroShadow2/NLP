{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дз №2 Кластеризация текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Emails = pd.read_csv(\"hillary-clinton-emails/Emails.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала выберем основные, на мой взгляд, атрибуты email сообщения. Соединяем через 'the' чтобы потом откинуть через stop_words и не получить в биграммы лишнего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                 WOW the Sullivan, Jacob J the H the \n",
       "1    H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...\n",
       "2     CHRIS STEVENS the Mills, Cheryl D the ;H the Thx\n",
       "3    CAIRO CONDEMNATION - FINAL the Mills, Cheryl D...\n",
       "4    H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...\n",
       "Name: Merged, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Emails[[\"MetadataSubject\", \"MetadataFrom\", \"MetadataTo\", \"ExtractedBodyText\"]]\n",
    "#' in '.join(map(str, list(data.xs(0))))\n",
    "data['Merged'] =[(' the '.join(map(str, list(data.xs(ind))))).replace('nan', '') for ind in data.index]\n",
    "data['Merged'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer=\"word\", stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7945, 244146)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = vectorizer.fit_transform(data['Merged'])\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим частые биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state gov\n",
      "abedin huma\n",
      "mills cheryl\n",
      "sullivan jacob\n",
      "millscd state\n"
     ]
    }
   ],
   "source": [
    "f_names = vectorizer.get_feature_names()\n",
    "f_counts = np.array(matrix.sum(axis = 0))\n",
    "#print matrix.shape, f_counts.shape\n",
    "#print f_counts\n",
    "for a,b in sorted([(f_counts[0][i], f_names[i]) for i in xrange(matrix.shape[1])])[::-1][:5]:\n",
    "    print str(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \" \".join(data['Merged'])\n",
    "#print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BHUTTO', 'ASSASSINATION'),\n",
       " ('CONSULAR', 'FEES'),\n",
       " ('Chico', 'Menashe'),\n",
       " ('Deep', 'Throat'),\n",
       " ('MARIE', 'SLAUGHTER'),\n",
       " ('BAILEY', 'HUTCHISON'),\n",
       " ('Belief', 'Blog'),\n",
       " ('Buenos', 'Aires'),\n",
       " ('DYING', 'PATIENTS'),\n",
       " ('Elliott', 'Trudeau')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tokens = nltk.wordpunct_tokenize(text)\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_freq_filter(5)\n",
    "finder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in ignored_words)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer1 = TfidfVectorizer(ngram_range=(2, 2), analyzer=\"word\", stop_words = 'english')\n",
    "f = vectorizer1.fit_transform(data['Merged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 5 5 5 5 5 5 3 5 3 5 3 5 5 5 5 5 3 3 0 5 5 0 3 5 3 5 5 2]\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "model = KMeans(n_clusters=n_clusters, random_state=1, n_jobs = -1)\n",
    "preds = model.fit_predict(f)\n",
    "print(preds[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посморим на размеры классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: 304\n",
      "#1: 483\n",
      "#2: 1295\n",
      "#3: 511\n",
      "#4: 95\n",
      "#5: 3971\n",
      "#6: 435\n",
      "#7: 370\n",
      "#8: 237\n",
      "#9: 244\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(n_clusters):\n",
    "    print \"#{}: {}\".format(i, len([x for x in preds if x == i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.860284280721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "clf = LogisticRegression()\n",
    "print(cross_val_score(clf, f, preds).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(f, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     best_positive                      ||                    best_negative                    \n",
      "#0 || sullivanjj state|   state gov    | sid sullivanjj ||  millscd state  | sullivan jacob | abedinh state  |\n",
      "#1 ||  millscd state  |   state gov    |valmorolj state || sullivanjj state|  mills cheryl  | abedinh state  |\n",
      "#2 ||   abedin huma   |  huma abedinh  |  calls abedin  ||  sullivan jacob |  mills cheryl  |   state gov    |\n",
      "#3 ||  sullivan jacob |   jacob fyi    | jacob sullivan ||   mills cheryl  |   state gov    |  abedin huma   |\n",
      "#4 || secretary office|   lauren ok    |     00 pm      ||   mills cheryl  |  abedin huma   |   state gov    |\n",
      "#5 ||  jiloty lauren  | vermarr state  |  sid sbwhoeop  ||  sullivan jacob |   state gov    |  abedin huma   |\n",
      "#6 ||   mills cheryl  |   cheryl fyi   |  update mills  ||   cheryl mills  |  abedin huma   |   state gov    |\n",
      "#7 ||  abedinh state  |   state gov    |     gov ok     ||  millscd state  |  mills cheryl  |  abedin huma   |\n",
      "#8 ||  jilotylc state |    gov pls     |   state gov    ||  millscd state  |  mills cheryl  |  abedin huma   |\n",
      "#9 ||   cheryl mills  | cheryl millscd |  mills cheryl  ||  abedinh state  |  abedin huma   |   cheryl fyi   |\n"
     ]
    }
   ],
   "source": [
    "print ('{:^56}||{:^53}').format(\"best_positive\", 'best_negative')\n",
    "for i in range(n_clusters):\n",
    "    cur_best= sorted([(coef, j) for j,coef in enumerate(clf.coef_[i])])[::-1]\n",
    "    print '#{:<2}||'.format(i),\n",
    "    print ('{:^16}|' * 3 + '|').format(*[f_names[j] for val, j in cur_best[:3]]),\n",
    "    print ('{:^16}|' * 3).format(*[f_names[j] for val, j in cur_best[-3:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили визуализацию в виде наиболее значимых признаков, отобранных логистической регрессией. По ней видно, что отрицательные коэффициенты, часто повторяются, что неудивительно. Среди положительных встречаются не только адресаты и отправители, что позволяет предположить, что кластеризация не самая тривиальная."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
